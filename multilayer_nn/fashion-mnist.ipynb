{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d71bce70-9dc3-448b-9f9a-8896e83b6d09",
   "metadata": {},
   "source": [
    "# Implementing a Multilayer Perceptron (MNIST)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b48fc7-4f46-4d5a-8558-cd06892aaa27",
   "metadata": {},
   "source": [
    "## 1) Installing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ea7b3b8-9092-4b37-8b7f-57362be611ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:25.036150Z",
     "start_time": "2023-10-18T15:40:21.316645Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (1.26.0)\n",
      "Requirement already satisfied: pandas in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: matplotlib in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (3.8.0)\n",
      "Requirement already satisfied: torch in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: torchvision in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (0.16.0)\n",
      "Requirement already satisfied: torchaudio in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (2.1.0)\n",
      "Requirement already satisfied: watermark in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (2.4.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from pandas) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (1.1.1)\n",
      "Requirement already satisfied: cycler>=0.10 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (4.43.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (23.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (10.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (3.1.1)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from matplotlib) (6.1.0)\n",
      "Requirement already satisfied: filelock in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torch) (3.12.4)\n",
      "Requirement already satisfied: typing-extensions in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torch) (4.8.0)\n",
      "Requirement already satisfied: sympy in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torch) (2023.9.2)\n",
      "Requirement already satisfied: requests in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: ipython>=6.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from watermark) (8.16.1)\n",
      "Requirement already satisfied: importlib-metadata>=1.4 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from watermark) (6.8.0)\n",
      "Requirement already satisfied: setuptools in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from watermark) (68.0.0)\n",
      "Requirement already satisfied: zipp>=0.5 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from importlib-metadata>=1.4->watermark) (3.17.0)\n",
      "Requirement already satisfied: backcall in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (0.2.0)\n",
      "Requirement already satisfied: decorator in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (0.1.6)\n",
      "Requirement already satisfied: pickleshare in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (3.0.39)\n",
      "Requirement already satisfied: pygments>=2.4.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (2.16.1)\n",
      "Requirement already satisfied: stack-data in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (5.11.2)\n",
      "Requirement already satisfied: exceptiongroup in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (1.1.3)\n",
      "Requirement already satisfied: colorama in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from ipython>=6.0->watermark) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from requests->torchvision) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from requests->torchvision) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from requests->torchvision) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from requests->torchvision) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from jedi>=0.16->ipython>=6.0->watermark) (0.8.3)\n",
      "Requirement already satisfied: wcwidth in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from prompt-toolkit!=3.0.37,<3.1.0,>=3.0.30->ipython>=6.0->watermark) (0.2.8)\n",
      "Requirement already satisfied: executing>=1.2.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from stack-data->ipython>=6.0->watermark) (2.0.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from stack-data->ipython>=6.0->watermark) (2.4.0)\n",
      "Requirement already satisfied: pure-eval in d:\\programming train\\dl_lightning_ai_course\\venv\\lib\\site-packages (from stack-data->ipython>=6.0->watermark) (0.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 23.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas matplotlib torch torchvision torchaudio watermark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bc4fa295-5c62-4888-bcf8-d07d6a7afc47",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:38.082290800Z",
     "start_time": "2023-10-18T15:40:25.036150Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python implementation: CPython\n",
      "Python version       : 3.9.4\n",
      "IPython version      : 8.16.1\n",
      "\n",
      "numpy     : 1.26.0\n",
      "pandas    : 2.1.1\n",
      "matplotlib: 3.8.0\n",
      "torch     : 2.1.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%load_ext watermark\n",
    "%watermark -v -p numpy,pandas,matplotlib,torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9549676-2fa5-41a7-bbb9-ce03f5797c34",
   "metadata": {},
   "source": [
    "## 2) Loading the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e002ad95-a1f7-4c33-826a-4a45944f2687",
   "metadata": {},
   "source": [
    "- MNIST website: http://yann.lecun.com/exdb/mnist/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f609024c-3eae-4ad5-8cb8-b95b403b7606",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:39.666262Z",
     "start_time": "2023-10-18T15:40:38.081766700Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    import os\n",
    "    import gzip\n",
    "    import numpy as np\n",
    "\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path,\n",
    "                               '%s-labels-idx1-ubyte.gz'\n",
    "                               % kind)\n",
    "    images_path = os.path.join(path,\n",
    "                               '%s-images-idx3-ubyte.gz'\n",
    "                               % kind)\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels\n",
    "\n",
    "for kind in (\"train\", \"t10k\"):\n",
    "    load_mnist(\"./fashion\", kind)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53652f7f-1327-4992-a78a-345921eeb854",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:39.682261200Z",
     "start_time": "2023-10-18T15:40:39.669262100Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None):\n",
    "\n",
    "        self.transform = transform\n",
    "        self._images = images\n",
    "        self._labels = labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = torch.from_numpy(self._images[index])\n",
    "        img = img.to(torch.float32)\n",
    "        img = img/255.\n",
    "        # ...\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = self._labels[index]\n",
    "\n",
    "        return img, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._labels.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "data_transforms = {\n",
    "    \"train\":  transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(32),\n",
    "            transforms.RandomCrop((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5, ), (0.5,)),\n",
    "        ]\n",
    "    ),\n",
    "    \"test\": transforms.Compose(\n",
    "        [\n",
    "            transforms.Resize(32),\n",
    "            transforms.RandomCrop((28, 28)),\n",
    "            transforms.ToTensor(),\n",
    "            transforms.Normalize((0.5,), (0.5,)),\n",
    "        ]\n",
    "    )\n",
    "}\n",
    "\n",
    "train_imgs, train_labels = load_mnist(\"./fashion\", \"train\")\n",
    "train_dataset = MyDataset(train_imgs, train_labels, transform=data_transforms[\"train\"])\n",
    "\n",
    "test_imgs, test_labels = load_mnist(\"./fashion\", \"t10k\")\n",
    "test_dataset = MyDataset(test_imgs, test_labels, transform=data_transforms[\"test\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.010405800Z",
     "start_time": "2023-10-18T15:40:39.686261200Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6661307a-6220-48d5-b965-4cd36e29e54c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.026409900Z",
     "start_time": "2023-10-18T15:40:40.011407200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "60000"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78adc94e-5418-4aac-9a82-9a9cbf8fc688",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.069482500Z",
     "start_time": "2023-10-18T15:40:40.027407400Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "10000"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "765adcf0-9147-434b-917a-f6d736a7117e",
   "metadata": {},
   "source": [
    "### Create a validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b39a42a2-cd46-46cf-ba93-d3f2f232f29c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.366822400Z",
     "start_time": "2023-10-18T15:40:40.043407Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data.dataset import random_split\n",
    "\n",
    "torch.manual_seed(1)\n",
    "train_dataset, val_dataset = random_split(train_dataset, lengths=[55000, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b30e4a70-55b3-4fb0-b28d-b0fddd193fae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.381824700Z",
     "start_time": "2023-10-18T15:40:40.368823300Z"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = DataLoader(\n",
    "    dataset=train_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=True,\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_dataset,\n",
    "    batch_size=64,\n",
    "    shuffle=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db50db02-3696-4f86-b149-74baabeec6c4",
   "metadata": {},
   "source": [
    "## 3) Implementing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "971389a7-5424-4141-a3ee-9399eebbbb6a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.413821400Z",
     "start_time": "2023-10-18T15:40:40.384823Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class PyTorchMLP(torch.nn.Module):\n",
    "    def __init__(self, num_features, num_classes):\n",
    "        super().__init__()\n",
    "\n",
    "        self.all_layers = torch.nn.Sequential(\n",
    "            # 1st hidden layer\n",
    "            torch.nn.Linear(num_features, 50),\n",
    "            torch.nn.ReLU(),\n",
    "            # 2nd hidden layer\n",
    "            torch.nn.Linear(50, 25),\n",
    "            torch.nn.ReLU(),\n",
    "            # output layer\n",
    "            torch.nn.Linear(25, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        logits = self.all_layers(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bc16a0-ec59-4c54-a209-0a5e22406287",
   "metadata": {},
   "source": [
    "## 4) The training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8de213fc-48b0-4f7c-af9e-8e2da068e351",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:40.415821900Z",
     "start_time": "2023-10-18T15:40:40.400823Z"
    }
   },
   "outputs": [],
   "source": [
    "def compute_accuracy(model, dataloader):\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    correct = 0.0\n",
    "    total_examples = 0\n",
    "\n",
    "    for idx, (features, labels) in enumerate(dataloader):\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(features)\n",
    "\n",
    "        predictions = torch.argmax(logits, dim=1)\n",
    "\n",
    "        compare = labels == predictions\n",
    "        correct += torch.sum(compare)\n",
    "        total_examples += len(compare)\n",
    "\n",
    "    return correct / total_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3dcaa2b1-4019-4128-9ff5-6a966c3abdf2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-18T15:40:41.650991200Z",
     "start_time": "2023-10-18T15:40:40.417823500Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aleks\\AppData\\Local\\Temp\\ipykernel_888\\3272151303.py:12: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\tensor_numpy.cpp:212.)\n",
      "  img = torch.from_numpy(self._images[index])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Tensor is not a torch image.",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 15\u001B[0m\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m epoch \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(num_epochs):\n\u001B[0;32m     14\u001B[0m     model \u001B[38;5;241m=\u001B[39m model\u001B[38;5;241m.\u001B[39mtrain()\n\u001B[1;32m---> 15\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m batch_idx, (features, labels) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(train_loader):\n\u001B[0;32m     17\u001B[0m         logits \u001B[38;5;241m=\u001B[39m model(features)\n\u001B[0;32m     19\u001B[0m         loss \u001B[38;5;241m=\u001B[39m F\u001B[38;5;241m.\u001B[39mcross_entropy(logits, labels)\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001B[0m, in \u001B[0;36m_BaseDataLoaderIter.__next__\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    627\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_sampler_iter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    628\u001B[0m     \u001B[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001B[39;00m\n\u001B[0;32m    629\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reset()  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[1;32m--> 630\u001B[0m data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_next_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    631\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    632\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_dataset_kind \u001B[38;5;241m==\u001B[39m _DatasetKind\u001B[38;5;241m.\u001B[39mIterable \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    633\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \\\n\u001B[0;32m    634\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_num_yielded \u001B[38;5;241m>\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_IterableDataset_len_called:\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001B[0m, in \u001B[0;36m_SingleProcessDataLoaderIter._next_data\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    672\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_next_data\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    673\u001B[0m     index \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_next_index()  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[1;32m--> 674\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_dataset_fetcher\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfetch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mindex\u001B[49m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# may raise StopIteration\u001B[39;00m\n\u001B[0;32m    675\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory:\n\u001B[0;32m    676\u001B[0m         data \u001B[38;5;241m=\u001B[39m _utils\u001B[38;5;241m.\u001B[39mpin_memory\u001B[38;5;241m.\u001B[39mpin_memory(data, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_pin_memory_device)\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:49\u001B[0m, in \u001B[0;36m_MapDatasetFetcher.fetch\u001B[1;34m(self, possibly_batched_index)\u001B[0m\n\u001B[0;32m     47\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mauto_collation:\n\u001B[0;32m     48\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m__getitems__\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__:\n\u001B[1;32m---> 49\u001B[0m         data \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m__getitems__\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpossibly_batched_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     50\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m     51\u001B[0m         data \u001B[38;5;241m=\u001B[39m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m possibly_batched_index]\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001B[0m, in \u001B[0;36mSubset.__getitems__\u001B[1;34m(self, indices)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx]] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\utils\\data\\dataset.py:364\u001B[0m, in \u001B[0;36m<listcomp>\u001B[1;34m(.0)\u001B[0m\n\u001B[0;32m    362\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdataset\u001B[38;5;241m.\u001B[39m__getitems__([\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mindices[idx] \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices])  \u001B[38;5;66;03m# type: ignore[attr-defined]\u001B[39;00m\n\u001B[0;32m    363\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 364\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdataset\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mindices\u001B[49m\u001B[43m[\u001B[49m\u001B[43midx\u001B[49m\u001B[43m]\u001B[49m\u001B[43m]\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m idx \u001B[38;5;129;01min\u001B[39;00m indices]\n",
      "Cell \u001B[1;32mIn[4], line 18\u001B[0m, in \u001B[0;36mMyDataset.__getitem__\u001B[1;34m(self, index)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;66;03m# ...\u001B[39;00m\n\u001B[0;32m     17\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransform \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m---> 18\u001B[0m     img \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtransform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m label \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_labels[index]\n\u001B[0;32m     22\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m img, label\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001B[0m, in \u001B[0;36mCompose.__call__\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m     93\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m     94\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m t \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtransforms:\n\u001B[1;32m---> 95\u001B[0m         img \u001B[38;5;241m=\u001B[39m \u001B[43mt\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     96\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m img\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torchvision\\transforms\\transforms.py:361\u001B[0m, in \u001B[0;36mResize.forward\u001B[1;34m(self, img)\u001B[0m\n\u001B[0;32m    353\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, img):\n\u001B[0;32m    354\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    355\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m    356\u001B[0m \u001B[38;5;124;03m        img (PIL Image or Tensor): Image to be scaled.\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    359\u001B[0m \u001B[38;5;124;03m        PIL Image or Tensor: Rescaled image.\u001B[39;00m\n\u001B[0;32m    360\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 361\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mresize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msize\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minterpolation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmax_size\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mantialias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:476\u001B[0m, in \u001B[0;36mresize\u001B[1;34m(img, size, interpolation, max_size, antialias)\u001B[0m\n\u001B[0;32m    470\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m max_size \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(size) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[0;32m    471\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    472\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmax_size should only be passed if size specifies the length of the smaller edge, \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    473\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mi.e. size should be an int or a sequence of length 1 in torchscript mode.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    474\u001B[0m         )\n\u001B[1;32m--> 476\u001B[0m _, image_height, image_width \u001B[38;5;241m=\u001B[39m \u001B[43mget_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    477\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(size, \u001B[38;5;28mint\u001B[39m):\n\u001B[0;32m    478\u001B[0m     size \u001B[38;5;241m=\u001B[39m [size]\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:76\u001B[0m, in \u001B[0;36mget_dimensions\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     74\u001B[0m     _log_api_usage_once(get_dimensions)\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(img, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m---> 76\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF_t\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_dimensions\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m F_pil\u001B[38;5;241m.\u001B[39mget_dimensions(img)\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:19\u001B[0m, in \u001B[0;36mget_dimensions\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     18\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_dimensions\u001B[39m(img: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m List[\u001B[38;5;28mint\u001B[39m]:\n\u001B[1;32m---> 19\u001B[0m     \u001B[43m_assert_image_tensor\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimg\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     20\u001B[0m     channels \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m img\u001B[38;5;241m.\u001B[39mndim \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m3\u001B[39m]\n\u001B[0;32m     21\u001B[0m     height, width \u001B[38;5;241m=\u001B[39m img\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m2\u001B[39m:]\n",
      "File \u001B[1;32mD:\\Programming Train\\dl_lightning_ai_course\\venv\\lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:15\u001B[0m, in \u001B[0;36m_assert_image_tensor\u001B[1;34m(img)\u001B[0m\n\u001B[0;32m     13\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_assert_image_tensor\u001B[39m(img: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     14\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_tensor_a_torch_image(img):\n\u001B[1;32m---> 15\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTensor is not a torch image.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mTypeError\u001B[0m: Tensor is not a torch image."
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "torch.manual_seed(1)\n",
    "model = PyTorchMLP(num_features=784, num_classes=10)\n",
    "\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "num_epochs = 30\n",
    "\n",
    "loss_list = []\n",
    "train_acc_list, val_acc_list = [], []\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    model = model.train()\n",
    "    for batch_idx, (features, labels) in enumerate(train_loader):\n",
    "\n",
    "        logits = model(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if not batch_idx % 250:\n",
    "            ### LOGGING\n",
    "            print(\n",
    "                f\"Epoch: {epoch+1:03d}/{num_epochs:03d}\"\n",
    "                f\" | Batch {batch_idx:03d}/{len(train_loader):03d}\"\n",
    "                f\" | Train Loss: {loss:.2f}\"\n",
    "            )\n",
    "        loss_list.append(loss.item())\n",
    "\n",
    "    train_acc = compute_accuracy(model, train_loader)\n",
    "    val_acc = compute_accuracy(model, val_loader)\n",
    "    print(f\"Train Acc {train_acc*100:.2f}% | Val Acc {val_acc*100:.2f}%\")\n",
    "    train_acc_list.append(train_acc)\n",
    "    val_acc_list.append(val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0d5821-7c8d-46b5-9e7d-02e72cac2acc",
   "metadata": {},
   "source": [
    "## 5) Evaluating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27538c8d-61bc-47b0-8289-b6aab4aa16ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_acc = compute_accuracy(model, train_loader)\n",
    "val_acc = compute_accuracy(model, val_loader)\n",
    "test_acc = compute_accuracy(model, test_loader)\n",
    "\n",
    "print(f\"Train Acc {train_acc*100:.2f}%\")\n",
    "print(f\"Val Acc {val_acc*100:.2f}%\")\n",
    "print(f\"Test Acc {test_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f1f8499-3191-4f78-b2d4-82daba9b9bc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_plotting import plot_training_loss\n",
    "\n",
    "plot_training_loss(minibatch_loss_list=loss_list,\n",
    "                   num_epochs=num_epochs,\n",
    "                   iter_per_epoch=len(loss_list)//num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d4519-b7a2-4382-90d0-a0dce2ee608e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_plotting import plot_accuracy\n",
    "\n",
    "plot_accuracy(train_acc_list=train_acc_list, valid_acc_list=val_acc_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
