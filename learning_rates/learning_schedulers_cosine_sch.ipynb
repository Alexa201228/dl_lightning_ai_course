{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-10-30T18:38:12.847716Z",
     "start_time": "2023-10-30T18:38:12.846234Z"
    }
   },
   "outputs": [],
   "source": [
    "import lightning as L\n",
    "import torch\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from tools import CustomDataModule, PyTorchMLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from pl_bolts.optimizers import LinearWarmupCosineAnnealingLR\n",
    "import torchmetrics\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class LightningModel(L.LightningModule):\n",
    "\n",
    "\n",
    "    def __init__(self, model, learning_rate, warmup_epochs, max_epochs):\n",
    "        super().__init__()\n",
    "\n",
    "        self.learning_rate = learning_rate\n",
    "        self.model = model\n",
    "        self.warmup_epochs = warmup_epochs\n",
    "        self.max_epochs = max_epochs\n",
    "\n",
    "        self.save_hyperparameters(ignore=[\"model\"])\n",
    "\n",
    "        self.train_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.val_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "        self.test_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def _shared_step(self, batch):\n",
    "        features, true_labels = batch\n",
    "        logits = self(features)\n",
    "\n",
    "        loss = F.cross_entropy(logits, true_labels)\n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        return loss, true_labels, predicted_labels\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"train_loss\", loss)\n",
    "        self.train_acc(predicted_labels, true_labels)\n",
    "        self.log(\n",
    "            \"train_acc\", self.train_acc, prog_bar=True, on_epoch=True, on_step=False\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss, true_labels, predicted_labels = self._shared_step(batch)\n",
    "\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        self.val_acc(predicted_labels, true_labels)\n",
    "        self.log(\"val_acc\", self.val_acc, prog_bar=True)\n",
    "\n",
    "    def test_step(self, batch, batch_idx):\n",
    "        _, true_labels, predicted_labels = self._shared_step(batch)\n",
    "        self.test_acc(predicted_labels, true_labels)\n",
    "        self.log(\"test_acc\", self.test_acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        scheduler = LinearWarmupCosineAnnealingLR(optimizer=optimizer, warmup_epochs=self.warmup_epochs, max_epochs=self.max_epochs)\n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"monitor\": \"train_loss\",\n",
    "                \"interval\": \"step\",\n",
    "                \"frequency\": 1,\n",
    "            },\n",
    "        }"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T18:38:12.857357Z",
     "start_time": "2023-10-30T18:38:12.856036Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "torch.manual_seed(12)\n",
    "d_module = CustomDataModule()\n",
    "d_module.setup(\"training\")\n",
    "\n",
    "num_epochs = 100\n",
    "\n",
    "pt_model = PyTorchMLP(num_features=100, num_classes=2)\n",
    "lt_model = LightningModel(model=pt_model, learning_rate=0.3, warmup_epochs=10, max_epochs=num_epochs)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T18:38:12.960695Z",
     "start_time": "2023-10-30T18:38:12.859836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "False"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T18:38:12.965320Z",
     "start_time": "2023-10-30T18:38:12.962931Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(\n",
    "    max_epochs=100,\n",
    "    accelerator=\"auto\",\n",
    "    devices=\"auto\",\n",
    "    logger=CSVLogger(save_dir=\"csv_logs/\", name=\"lt_model\"),\n",
    "    deterministic=True,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T18:38:13.009548Z",
     "start_time": "2023-10-30T18:38:12.971949Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/s7/3gl6smcn25z7fl4g6g34mszr0000gn/T/ipykernel_3725/500208232.py:57: UnderReviewWarning: The feature LinearWarmupCosineAnnealingLR is currently marked under review. The compatibility with other Lightning projects is not guaranteed and API may change at any time. The API and functionality may change without warning in future releases. More details: https://lightning-bolts.readthedocs.io/en/latest/stability.html\n",
      "  scheduler = LinearWarmupCosineAnnealingLR(optimizer=optimizer, warmup_epochs=self.warmup_epochs, max_epochs=self.max_epochs)\n",
      "\n",
      "  | Name      | Type               | Params\n",
      "-------------------------------------------------\n",
      "0 | model     | PyTorchMLP         | 15.3 K\n",
      "1 | train_acc | MulticlassAccuracy | 0     \n",
      "2 | val_acc   | MulticlassAccuracy | 0     \n",
      "3 | test_acc  | MulticlassAccuracy | 0     \n",
      "-------------------------------------------------\n",
      "15.3 K    Trainable params\n",
      "0         Non-trainable params\n",
      "15.3 K    Total params\n",
      "0.061     Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Sanity Checking: 0it [00:00, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "63be06a2fa5046d7b3c69f6d2c90bc40"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "RuntimeError",
     "evalue": "MPS backend out of memory (MPS allocated: 59.75 KB, other allocations: 18.13 GB, max allowed: 18.13 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:38\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     37\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m---> 38\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtrainer_fn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m _TunerExitException:\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:650\u001B[0m, in \u001B[0;36mTrainer._fit_impl\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    644\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_ckpt_path \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39m_set_ckpt_path(\n\u001B[1;32m    645\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mfn,\n\u001B[1;32m    646\u001B[0m     ckpt_path,  \u001B[38;5;66;03m# type: ignore[arg-type]\u001B[39;00m\n\u001B[1;32m    647\u001B[0m     model_provided\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    648\u001B[0m     model_connected\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    649\u001B[0m )\n\u001B[0;32m--> 650\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mckpt_path\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    652\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstopped\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1112\u001B[0m, in \u001B[0;36mTrainer._run\u001B[0;34m(self, model, ckpt_path)\u001B[0m\n\u001B[1;32m   1110\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_checkpoint_connector\u001B[38;5;241m.\u001B[39mresume_end()\n\u001B[0;32m-> 1112\u001B[0m results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_stage\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1114\u001B[0m log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: trainer tearing down\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1191\u001B[0m, in \u001B[0;36mTrainer._run_stage\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1190\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_run_predict()\n\u001B[0;32m-> 1191\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_train\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1204\u001B[0m, in \u001B[0;36mTrainer._run_train\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1203\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m isolate_rng():\n\u001B[0;32m-> 1204\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_run_sanity_check\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1206\u001B[0m \u001B[38;5;66;03m# enable train mode\u001B[39;00m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1269\u001B[0m, in \u001B[0;36mTrainer._run_sanity_check\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1268\u001B[0m \u001B[38;5;66;03m# reload dataloaders\u001B[39;00m\n\u001B[0;32m-> 1269\u001B[0m \u001B[43mval_loop\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reload_evaluation_dataloaders\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1270\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_sanity_val_batches \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m   1271\u001B[0m     \u001B[38;5;28mmin\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_sanity_val_steps, val_batches) \u001B[38;5;28;01mfor\u001B[39;00m val_batches \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_val_batches\n\u001B[1;32m   1272\u001B[0m ]\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/loops/dataloader/evaluation_loop.py:234\u001B[0m, in \u001B[0;36mEvaluationLoop._reload_evaluation_dataloaders\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    233\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mval_dataloaders \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39m_data_connector\u001B[38;5;241m.\u001B[39m_should_reload_val_dl:\n\u001B[0;32m--> 234\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mreset_val_dataloader\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    235\u001B[0m     dataloaders \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtrainer\u001B[38;5;241m.\u001B[39mval_dataloaders\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1649\u001B[0m, in \u001B[0;36mTrainer.reset_val_dataloader\u001B[0;34m(self, model)\u001B[0m\n\u001B[1;32m   1647\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_last_val_dl_reload_epoch \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcurrent_epoch\n\u001B[0;32m-> 1649\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnum_val_batches, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mval_dataloaders \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data_connector\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_reset_eval_dataloader\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   1650\u001B[0m \u001B[43m    \u001B[49m\u001B[43mRunningStage\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mVALIDATING\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpl_module\u001B[49m\n\u001B[1;32m   1651\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:391\u001B[0m, in \u001B[0;36mDataConnector._reset_eval_dataloader\u001B[0;34m(self, mode, model)\u001B[0m\n\u001B[1;32m    389\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i, dataloader \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(dataloaders):\n\u001B[1;32m    390\u001B[0m     orig_num_batches \u001B[38;5;241m=\u001B[39m num_batches \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 391\u001B[0m         \u001B[38;5;28mlen\u001B[39m(dataloader) \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mhas_len_all_ranks\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdataloader\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodule\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mfloat\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minf\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    392\u001B[0m     )\n\u001B[1;32m    394\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m orig_num_batches \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/utilities/data.py:109\u001B[0m, in \u001B[0;36mhas_len_all_ranks\u001B[0;34m(dataloader, strategy, model)\u001B[0m\n\u001B[1;32m    107\u001B[0m total_length \u001B[38;5;241m=\u001B[39m strategy\u001B[38;5;241m.\u001B[39mreduce(torch\u001B[38;5;241m.\u001B[39mtensor(local_length, device\u001B[38;5;241m=\u001B[39mstrategy\u001B[38;5;241m.\u001B[39mroot_device), reduce_op\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msum\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 109\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[43mtotal_length\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;241;43m0\u001B[39;49m:\n\u001B[1;32m    110\u001B[0m     rank_zero_warn(\n\u001B[1;32m    111\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mTotal length of `\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mdataloader\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m` across ranks is zero.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    112\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m Please make sure this was your intention.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    113\u001B[0m     )\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 122.00 KB, other allocations: 18.13 GB, max allowed: 18.13 GB). Tried to allocate 256 bytes on shared pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure).",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001B[0;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[24], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlt_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43md_module\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:608\u001B[0m, in \u001B[0;36mTrainer.fit\u001B[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001B[0m\n\u001B[1;32m    606\u001B[0m model \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_maybe_unwrap_optimized(model)\n\u001B[1;32m    607\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstrategy\u001B[38;5;241m.\u001B[39m_lightning_module \u001B[38;5;241m=\u001B[39m model\n\u001B[0;32m--> 608\u001B[0m \u001B[43mcall\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call_and_handle_interrupt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    609\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_fit_impl\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtrain_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mval_dataloaders\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdatamodule\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mckpt_path\u001B[49m\n\u001B[1;32m    610\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/call.py:63\u001B[0m, in \u001B[0;36m_call_and_handle_interrupt\u001B[0;34m(trainer, trainer_fn, *args, **kwargs)\u001B[0m\n\u001B[1;32m     61\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m logger \u001B[38;5;129;01min\u001B[39;00m trainer\u001B[38;5;241m.\u001B[39mloggers:\n\u001B[1;32m     62\u001B[0m     logger\u001B[38;5;241m.\u001B[39mfinalize(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mfailed\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 63\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_teardown\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[38;5;66;03m# teardown might access the stage so we reset it after\u001B[39;00m\n\u001B[1;32m     65\u001B[0m trainer\u001B[38;5;241m.\u001B[39mstate\u001B[38;5;241m.\u001B[39mstage \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/trainer/trainer.py:1175\u001B[0m, in \u001B[0;36mTrainer._teardown\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1172\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_teardown\u001B[39m(\u001B[38;5;28mself\u001B[39m) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   1173\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"This is the Trainer's internal teardown, unrelated to the `teardown` hooks in LightningModule and\u001B[39;00m\n\u001B[1;32m   1174\u001B[0m \u001B[38;5;124;03m    Callback; those are handled by :meth:`_call_teardown_hook`.\"\"\"\u001B[39;00m\n\u001B[0;32m-> 1175\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstrategy\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mteardown\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1176\u001B[0m     loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_active_loop\n\u001B[1;32m   1177\u001B[0m     \u001B[38;5;66;03m# loop should never be `None` here but it can because we don't know the trainer stage with `ddp_spawn`\u001B[39;00m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/pytorch/strategies/strategy.py:496\u001B[0m, in \u001B[0;36mStrategy.teardown\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    494\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlightning_module \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m    495\u001B[0m     log\u001B[38;5;241m.\u001B[39mdetail(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: moving model to CPU\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 496\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlightning_module\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    497\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprecision_plugin\u001B[38;5;241m.\u001B[39mteardown()\n\u001B[1;32m    498\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/lightning/fabric/utilities/device_dtype_mixin.py:78\u001B[0m, in \u001B[0;36m_DeviceDtypeModuleMixin.cpu\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m     76\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"See :meth:`torch.nn.Module.cpu`.\"\"\"\u001B[39;00m\n\u001B[1;32m     77\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m__update_properties(device\u001B[38;5;241m=\u001B[39mtorch\u001B[38;5;241m.\u001B[39mdevice(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcpu\u001B[39m\u001B[38;5;124m\"\u001B[39m))\n\u001B[0;32m---> 78\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/torch/nn/modules/module.py:967\u001B[0m, in \u001B[0;36mModule.cpu\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    958\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcpu\u001B[39m(\u001B[38;5;28mself\u001B[39m: T) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m T:\n\u001B[1;32m    959\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124mr\u001B[39m\u001B[38;5;124;03m\"\"\"Moves all model parameters and buffers to the CPU.\u001B[39;00m\n\u001B[1;32m    960\u001B[0m \n\u001B[1;32m    961\u001B[0m \u001B[38;5;124;03m    .. note::\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    965\u001B[0m \u001B[38;5;124;03m        Module: self\u001B[39;00m\n\u001B[1;32m    966\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 967\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43;01mlambda\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcpu\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/torch/nn/modules/module.py:810\u001B[0m, in \u001B[0;36mModule._apply\u001B[0;34m(self, fn, recurse)\u001B[0m\n\u001B[1;32m    808\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[1;32m    809\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[0;32m--> 810\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    812\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[1;32m    813\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[1;32m    814\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[1;32m    815\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    820\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[1;32m    821\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[0;32m~/ProgrammingTools/miniconda3/envs/dl_lightning_ai_course/lib/python3.10/site-packages/torchmetrics/metric.py:808\u001B[0m, in \u001B[0;36mMetric._apply\u001B[0;34m(self, fn, exclude_state)\u001B[0m\n\u001B[1;32m    802\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(\n\u001B[1;32m    803\u001B[0m             \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected metric state to be either a Tensor or a list of Tensor, but encountered \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mcurrent_val\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    804\u001B[0m         )\n\u001B[1;32m    806\u001B[0m \u001B[38;5;66;03m# make sure to update the device attribute\u001B[39;00m\n\u001B[1;32m    807\u001B[0m \u001B[38;5;66;03m# if the dummy tensor moves device by fn function we should also update the attribute\u001B[39;00m\n\u001B[0;32m--> 808\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_device \u001B[38;5;241m=\u001B[39m fn(\u001B[43mtorch\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mzeros\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m1\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mdevice\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m)\u001B[38;5;241m.\u001B[39mdevice\n\u001B[1;32m    810\u001B[0m \u001B[38;5;66;03m# Additional apply to forward cache and computed attributes (may be nested)\u001B[39;00m\n\u001B[1;32m    811\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m this\u001B[38;5;241m.\u001B[39m_computed \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "\u001B[0;31mRuntimeError\u001B[0m: MPS backend out of memory (MPS allocated: 59.75 KB, other allocations: 18.13 GB, max allowed: 18.13 GB). Tried to allocate 256 bytes on private pool. Use PYTORCH_MPS_HIGH_WATERMARK_RATIO=0.0 to disable upper limit for memory allocations (may cause system failure)."
     ]
    }
   ],
   "source": [
    "trainer.fit(model=lt_model, datamodule=d_module)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T18:38:13.264907Z",
     "start_time": "2023-10-30T18:38:13.004929Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "metrics = pd.read_csv(f\"{trainer.logger.log_dir}/metrics.csv\")\n",
    "print(metrics.head())\n",
    "aggr_metrics = []\n",
    "agg_col = \"epoch\"\n",
    "\n",
    "for i, dfg in metrics.groupby(agg_col):\n",
    "\n",
    "    agg = dict(dfg.mean())\n",
    "    agg[agg_col] = i\n",
    "    aggr_metrics.append(agg)\n",
    "\n",
    "df_metrics = pd.DataFrame(aggr_metrics)\n",
    "print(df_metrics.head())\n",
    "df_metrics[[\"train_loss\", \"val_loss\"]].plot(\n",
    "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"loss\"\n",
    ")\n",
    "df_metrics[[\"train_acc\", \"val_acc\"]].plot(\n",
    "    grid=True, legend=True, xlabel=\"Epoch\", ylabel=\"ACC\"\n",
    ")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "trainer.test(model=lt_model, datamodule=d_module)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(pt_model.parameters(), lr=0.1)\n",
    "sch = torch.optim.lr_scheduler.StepLR(opt, step_size=10, gamma=0.5)\n",
    "\n",
    "lrs = []\n",
    "max_epochs = 100\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    opt.step()\n",
    "    lrs.append(opt.param_groups[0][\"lr\"])\n",
    "    sch.step()\n",
    "\n",
    "plt.plot(range(max_epochs), lrs)\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Learning rate\")\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
